\section{Related Work}
\label{sec:relatedWork}

\paragraph{Conceptual models and taxonomy:}
\citet{Thum2014} established the taxonomy for product-line analyses upon which we based our work, that is, the classification
of analysis techniques in three basic strategies (product-based, feature-based, and family-based) and combinations thereof.
Furthermore, \citet{AnalysisToolsSurvey} surveyed existing product-line analysis tools and categorized them along four criteria:
product-line implementation technique (annotation-based \textit{versus} composition-based approach),
analysis technique (e.g., testing, type checking, model checking),
strategies for product-line analysis (i.e., the analysis strategies taxonomy by \citet{Thum2014}),
and strategy of the tool (product-based, variability-aware, and variability-encoding).
In this work, we build upon these existing taxonomies to propose a framework relating
analysis \emph{steps} in all dimensions.
Moreover, although the surveys by \citet{Thum2014} and \citet{AnalysisToolsSurvey}
range over a larger amount of primary studies, our work establishes finer-grained
relationships between analysis steps, supported by formal reasoning.
In principle, our framework could be applied to describe the studies surveyed by
\citet{Thum2014} and \citet{AnalysisToolsSurvey}---for instance,
the ones that are part of the qualitative analysis in
\Cref{sec:instance-splverifier,sec:instance-spllift}.

\citet{PLAModel} proposed the PLA model, which the authors argue is a
formal model for describing and comparing product-line analyses.
That model consists of four operators that express possible manipulations
of product line artifacts during analysis.
Our commuting diagram  (\Cref{fig:strategies-generic}) relates  to the PLA model as follows:
all downward arrows are instances of \emph{processing step},
all straight arrows from either left or right to the center are instances of \emph{variability restriction}, and
all arcs from left to right are instances of \emph{variability combinator}.
Moreover, whereas \citet{PLAModel} provide static building blocks for
describing product line analyses, we present structurally related
analysis steps and formalize conditions for their applicability.

\citet{SPLAnalysisTime} discussed the analysis of product lines throughout their
life cycle.
The authors reviewed product-line analysis techniques that can be applied to
regression analysis and, conversely, revision analyses that can be leveraged in a
space-variant setting.
Then, \citet{SPLAnalysisTime} conjectured that analyses of product-line variations
in time (i.e., evolution) could be modeled as a fourth dimension in the PLA
model~\cite{PLAModel}, combining both types of techniques.
Within the scope of such modeling effort, we envision that our work can be
extended to cover the time dimension as well, leveraging the finer-grained (yet generic) analysis steps.

% existing formalization x we generalize and identify key concepts and assumptions
\paragraph{Formal approaches to variability-aware analysis:}
The definition of product-line analysis techniques that are sound by construction has been investigated
recently~\cite{Midtgaard2015,ErwigAnalysis,Intraprocedural} in different contexts.
\citet{Midtgaard2015} presented a methodology to systematically derive family-based static analyses from single-product analyses
based on \emph{abstract interpretation};
\citet{ErwigAnalysis} defined a framework for automatic lifting of
static analyses that are expressible as type systems;
and \citet{Intraprocedural} proposed a
technique to automatically lift \emph{intraprocedural data-flow analyses} to handle
variability in product lines.
In contrast to their work, we provide a basis for structuring proofs of correctness
without constraining them to a specific analysis technique formalism.
On the other hand, we expect users of our framework to perform more formalization
activities to bridge their concrete setting to our abstract one.
Moreover, whereas \citet{Midtgaard2015}, \citet{ErwigAnalysis}, and
\citet{Intraprocedural} handle only the family-based
dimension of analysis,
we also address the feature-based dimension.

%\citet{StaticAnalysisInPractice} focus on the practical aspects of static analysis by implementing variability-aware control-flow and data-flow analyses for highly configurable systems, based on the \textsc{TypeChef} tooling infrastructure. Their evaluation shows that the work scales to real-world systems and the performance is comparable to that of sampling techniques. They focus on the family-based dimension and employ sharing techniques to leverage existing analyses in the variability context.

Through their seminal work on FTS, Classen et al. \cite{Classen2013,Classen2014} laid the foundations for designing product-line model checking strategies.
\citet{Classen2013} were concerned with annotative strategies (i.e., the right-hand side of our framework diagram). The principles of their strategies were later reused and extended to solve automata-based verification problems (e.g. real-time model checking \cite{Cordy2013}, 2-player games \cite{Greenyer2013b}).
\citet{Classen2014} proved the equivalence between compositional and annotative FTS (i.e., the existence of the encoding function $\gamma$ for these models). Our framework highlights that purely compositional strategies (i.e., the left-hand side of our diagram) have not been investigated for FTS.

Earlier work on product-line model checking \cite{Fantechi2008} represents the behaviour of multiple products as modal automata, i.e., automata with optional and mandatory transitions. While such modeling allows checking that given properties hold for the whole product line, they cannot trace back the features responsible for property violations. Framing this verification problem within our framework would indeed highlight the impossibility to construct annotative (lifted) expressions. This limitation was later circumvented by extending modal automata with variability constraints \cite{Asirelli2011,TerBeek2016}. This makes modal automata as expressive as FTS \cite{TerBeek2015,TerBeek2019} and paves the way for exploiting the benefits of both formalisms \cite{Varshosaz2019}.

% In contrast to their work, we provide a basis for structuring proofs of correctness
% without constraining them to a specific analysis technique formalism.
% On the other hand, we expect users of our framework to perform more formalization
% activities to bridge their concrete setting to our abstract one.
% Moreover, whereas \citet{Midtgaard2015}, \citet{ErwigAnalysis},
% \citet{Intraprocedural}, and \citet{StaticAnalysisInPractice} handle only the family-based
% dimension of analysis,
% we also address the feature-based dimension.

\citet{Dimovski2019} proposed a formal approach to apply variability-aware analyses even
in the case where they are not immediately feasible.
Given a variability-aware analysis, this technique searches for a suitable abstraction
that allows a pre-analysis to be performed.
Such pre-analysis, in its turn, is used to find out the features which have the same effect on
the property under evaluation (and thus can be grouped) and those that are irrelevant to the
problem at hand (and can be ignored).
The work by \citet{Dimovski2019} handles the optimization of already lifted analyses, whereas
our own handles aspects of the variability-aware analysis itself.
Hence, our understanding is that both approaches are complementary.
Moreover, similar to this work, \citet{Dimovski2019} propose that Binary Decision Diagrams
be used to increase sharing of analysis results.

\citet{Castro2017} formalized strategies for user-oriented reliability analysis of
product lines, covering all possible combinations in the taxonomy by \citet{Thum2014}.
Their work presented mathematical specifications and manual soundness proofs of these
strategies, along with a commuting diagram relating their intermediate steps.
In contrast, our work does not formalize concrete analysis techniques, but provides
a machine-verified theory regarding generic concepts involved in product-line
analyses.
Nonetheless, we used the commuting diagram by \citet{Castro2017} as a starting point
to elicit candidate reusable concepts (cf. \Cref{sec:reliability}).

\citet{generic-semantics-fm} presented a formalization of feature diagram semantics.
In doing so, the authors defined feature diagrams in a precise manner, thereby establishing a formal relationship between existing notations.
Likewise, our framework aims to formally define concepts that are otherwise expressed using natural language.
However, whereas \citet{generic-semantics-fm} deal with formalization and analysis of \emph{variability management} artifacts, our work addresses the analysis of properties of \emph{derivable products}.

\citet{StaticAnalysisInPractice} handled practical aspects of static analysis by implementing variability-aware control-flow and data-flow analyses for large-scale and highly configurable systems, based on the \textsc{TypeChef} tooling infrastructure.
Their evaluation shows the applicability of variability-aware analysis to real-world systems, with performance comparable to that of sampling techniques.
To achieve such results, the authors focus on the family-based dimension and employ sharing techniques to leverage existing analyses in the variability context.
Similar to our work, \citet{StaticAnalysisInPractice} present formal definitions of concepts necessary for the presented techniques.
However, the authors do not present proofs of correctness.
Given the relevance of analyzing industrial systems, we regard such an effort as an important step towards ensuring that the results can be trusted.

Last, we note that all aforementioned approaches to variability-aware analysis deal with concrete techniques for computing specific properties.
Our work, in contrast, abstracts from such details in pursuit of a general framework that can be reused in different scenarios.
Moreover, the definitions and theorems in that related work are manually crafted, whereas the concepts presented in this work are specified and proved using a proof assistant, which further increases confidence in the results.

\paragraph{Mechanized specification of product lines:}
Other researchers have leveraged theorem provers and proof assistants in
the context of software product lines (e.g.,
\citet{Teixeira2015,ThumProofComposition,DelawareTheoremPL,BorbaPLRefinement,Neves2011, SampaioJSS19}).
However, most of the existing work investigates the reuse of specification
and proofs to
assert soundness of different products in a given product line (product
lines of theorems).
Our work, in contrast, deals with properties of product lines in general.

\citet{BorbaPLRefinement} also specified a PVS theory about properties of
product lines---in their case, for reasoning about safe product-line
evolution.
That work evolved into a product line of theories \cite{Teixeira2015},
where products are theories of safe evolution based on concrete
product-line languages.
Similar to our results, their work present PVS theories about properties of
product lines.
Later, \citet{SampaioJSS19} extended the refinement theory to contemplate changes that do not preserve the behavior of the entire set of products in a product line, thus establishing the notion of partially safe evolution of product lines. All of their properties and theories are also specified and proved using PVS. 
Nonetheless, \citet{Neves2011}, \citet{SampaioJSS19} and \citet{Teixeira2015} specified
concepts in the domain of product-line engineering, whereby the targets of
their theories are meta-models of product lines.
Our work focuses on properties of product-line analysis strategies, instead.

%TODO check if this should be in the previous subsection
\citet{FLAME:SoSym} proposed the FaMa formaL frAMEwork (FLAME), which comprises a formalization of analysis operation over variability models, together with a reference implementation in Prolog. The semantics of different analysis operation were specified using Z, and defined over a common abstract layer to different variability model notations. Our goal is similar in the sense that we aim to abstract analysis operations and representations. However, we go beyond \emph{variability model} analysis, and we use a mechanized theorem prover to specify our theories.

\paragraph{Product line of theories:}

To leverage the machine-verified theory presented in
this work, one needs to instantiate uninterpreted
elements to obtain a concrete setting (property,
product, and analysis technique).
By doing so, our generalized theory of product-line
analysis becomes itself a product line of mechanized
theories.

\citet{Teixeira2015} presented a solution to a similar problem.
The authors created a generic theory to reason about product-line evolution,
based on a refinement notion that is independent of the concrete languages which
may be used to manage variability in a product line.
Then, \citet{Teixeira2015} employed product-line engineering techniques and
leveraged the theory interpretation mechanism in PVS to systematically reuse
soundness proofs in concrete scenarios.
Similar to our theory, their work provides a generic ``backbone'' and requires
instantiations to be manually developed.
However, at this point we do not provide a means to systematically manage the reuse
of specifications and proofs from the generic theory.
We plan to do so by employing similar techniques to the ones used by \citet{Teixeira2015}.

Another possible approach to manage a family of theories is that proposed by
\citet{DelawareTheoremPL}.
In that work, the authors presented a means to manage features of programming languages
along with corresponding theorems for reasoning about them.
The specification and proofs for each feature are contained in a Coq module, and the modules
containing selected features are manually imported and used.
This approach can be classified as a bottom-up composition, whereas our work
establishes a generic theory to be instantiated in a top-down fashion.