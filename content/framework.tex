\section{A Formal Framework for Software Product Line Analysis}
\label{sec:generalStructure}

In this section, we generalize and formalize the discussion of Section~\ref{sec:strategies}. We first present an overview of the resulting analysis framework (Section~\ref{sec:fmkoverview}) and formalize key aspects (Section~\ref{sec:abstraction-description}). 

\subsection{Analysis Framework Overview}
\label{sec:fmkoverview}

The framework aims at precisely and uniformly describing product-line analyses and their key properties. To create the framework, we reviewed existing analyses for the different models and properties mentioned in Section~\ref{sec:strategies} and identified essential abstractions of such analyses and their structure. 
Specifically, we started from concrete models and properties, such as the reliability analysis mentioned in~\Cref{sec:reliability}. We then started a process of formulating more general concepts by means of abstraction. This also required assessing impact on dependent elements, which also had to be rewritten. Finally, we also identified assumptions that such elements should fulfill to keep the diagram's structure.

For example, in Figure~\ref{fig:strategies-overview}, by abstracting \textit{DTMC} into \textit{Product}, derivation by projection ($\pi$) also needs to be abstracted, so that $\pi$'s domain becomes a generic \textit{Product} model with annotations, its co-domain becomes \textit{Product}, and its semantics is generalized to bind variability in the generic \textit{Product} model with annotations. 
The goal is that the framework accommodates, at least, the analyses of Section~\ref{sec:strategies}, and becomes a generic theory, consisting of a structure of key concepts related to product-line analyses, to be further evaluated with instantiations for different product models and properties, as discussed in Section~\ref{sec:frameworkInstances}.

\begin{figure}[!htbp]
	\centering
    \includegraphics{figures/generic-diagram.tikz}
	\caption{Software product line analysis framework}
	\label{fig:strategies-generic}
\end{figure}

Figure~\ref{fig:strategies-generic} and Table~\ref{table:analysis-abstraction-framework} synthesize the outcome of this abstraction process. 
At a coarse grain, Figure~\ref{fig:strategies-generic} is a generic diagram abstracting the structures presented in  Figures~\ref{fig:strategies-overview} and~\ref{fig:diagram-FTS}. At a fine grain, Table~\ref{table:analysis-abstraction-framework} relates concrete elements of Figures~\ref{fig:strategies-overview} and~\ref{fig:diagram-FTS} to corresponding generic elements in Figure~\ref{fig:strategies-generic}, together with a brief 
description of the latter elements in the generic framework.
These generic elements correspond to key abstractions of both the taxonomy proposed by
\citet{Thum2014} and of the product-line representation by \citet{Kastner2008}, which have been used to describe and analyze a multitude of product lines~\cite{Thum2014}.

%% @vra: removed since sounds a bit repetitive at this point in the paper
%As an overview of the framework, in the simplest case, we analyze a variability-free model \textit{Product} using some standard (non-variability aware) analysis technique $\alpha$.
%To perform an analysis of a given product line, one starts with a compositional (top-left corner 
%in Figure~\ref{fig:strategies-generic}) or an annotative product line (top-right corner), then follows 
%any of the outgoing arrows while performing the respective analysis steps, 
%until the desired properties are computed (either product-wise or as an ADD representing all possible values). 
%Each path from a compositional or annotative product line until a property value has been computed defines an analysis strategy, 
%which amounts to function composition of the intermediate analysis steps (arrows in the diagram). 
%The nodes in the diagram represent models or abstractions thereof after application of such analysis steps.

The diagram shown in Figure~\ref{fig:strategies-generic} offers different ways to analyze a given \textit{Property} of a product line of \textit{Product} members in a number of ways. 
We can obtain a product (variability-free model) by \textit{projection} ($\pi$) in annotative product lines or via model \textit{composition} ($\pi'$), in the case of compositional product lines, then  proceed with a non-variability-aware analysis $\alpha$.
Alternatively, for annotative product lines, we can apply a \textit{variability-aware analysis} $\mathit{\hat{\alpha}}$ to obtain an \textit{annotative expression}, which is either evaluated for a given product ($\sigma$) or further abstracted into an \textit{annotative lifted expressions} ($\mathit{lift}$). The latter is evaluated into a concise representation as an ADD ($\hat{\sigma}$). 

In the case of compositional product lines, \textit{variability-aware analysis} is applied to the compositional model in a structure-preserving 
way ($\mathit{fmap(\hat{\alpha})}$), yielding a \textit{compositional expression}, which is again either evaluated for a given product ($\sigma$) or further abstracted into a \textit{compositional lifted expression} ($\mathit{fmap(lift)}$) for later evaluation into an ADD ($\hat{\sigma}$). 

In either case, the purpose of lift operations is to provide a representation of expressions enabling their non-enumerative evaluation (cf. \Cref{sec:ADD}), which is key to the family-based dimension of the analysis.
Furthermore, an ADD-based representation is often used for a space-efficient encoding of values and also encompasses a BDD-based representation, since the value nodes in an ADD can represent values other than Booleans. 

Compositional product lines and compositional expressions can be transformed into corresponding annotative versions by means of \textit{variability encoding} ($\gamma$). In addition to the examples presented in Section~\ref{sec:strategies}, this is also possible for Lightweight Java~\cite{LJava}, as has been shown elsewhere~\cite{gamaLJ}, whereby complete refactorings enable transforming physical separation of features (compositional product line) to their virtual separation counterpart (annotative product line).



Similarly to Figures~\ref{fig:strategies-overview} and~\ref{fig:diagram-FTS}, each path from a compositional or annotative product line until a property value has been computed defines an analysis strategy, which amounts to function composition of the intermediate analysis steps (arrows in the diagram). The nodes in the diagram represent models or abstractions thereof after applying a series of analysis steps. 
These nodes and their underlying structure are defined in Section~\ref{sec:abstraction-description} and further discussed in Section~\ref{framework:discussion}.

As a result, the analyses are presented in a compositional manner in the form of a conceptual framework, 
which provides an overall understanding of how the different types of product-line analyses compose 
and inter-relate.  For instance, the feature-family-based analysis (in Figure~\ref{fig:strategies-generic}, 
from compositional product line, down, down, and right) and the feature-product-based analysis (from compositional product line, 
down, and right) share the feature dimension of the analysis (down from compositional product line). 
Furthermore, based on Figures~\ref{fig:strategies-overview} and~\ref{fig:diagram-FTS}, one could conjecture that the diagram in Figure~\ref{fig:strategies-generic} is also a  commuting diagram: different analysis paths yield equal results if they share the start and end points. In fact, this is indeed the case, according to the framework formalization we describe in the following section. %Section~\ref{sec:abstraction-description}.



%\begin{landscape}
\begin{table}[htb]
	\centering
	
	\caption{Synthesis of abstraction process defining our software product line analysis framework (element view)}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{llll}
			\toprule
			
\textbf{Framework Element} & \textbf{Description} &\textbf{Reliability Analysis Element} &  \textbf{Qualitative Temporal Logic} \\\midrule
Property & Computable property & Reliability & Temporal logic property \\
Product & Variability-free model &			  DTMC & Transition Systems \\
Analysis ($\alpha$) &  Variability-free analysis &			  Model checking  & Model checking \\
Annotative Product Line & SPL with annotative representation &			  Annotative probabilistic model & Featured Transition Systems \\
Compositional Product Line & SPL with compositional representation &			  Compositional probabilistic model & SMV+fSMV \\
Annotative expression & Expression computing property value at products  &			  Annotative rational expressions  & Feature expression \\
Compositional expression & Compositional counterpart of annotative expressions  &			  Compositional rational expressions  & -- \\
Annotative lifted expression & ADD-encoded annotative  expressions &			  Annotative lifted expression & Lifted feature expression \\
Compositional lifted expression & ADD-encoded compositional  expressions &			  Compositional lifted expressions & -- \\
Property ADD & Maps product configurations to property values &		  Reliability ADD & BDD \\
Projection ($\pi$)  & Product derivation by presence condition  projection &			  PMC projection  & projection \\
Composition ($\pi'$) & Product derivation by model composition &			  PMC composition  & feature composition \\
Variability-aware analysis ($\hat{\alpha}$) & SPL analysis exploring commonalities (sharing) &			  Parametric model checking  & FTS model checking  \\
Evaluation ($\sigma$) & Expression evaluation to a property value &	rational expression	evaluation  & Product evaluation \\
Evaluation with ADDs ($\hat{\sigma}$)  & Expression evaluation to an ADD &			  ADD evaluation  & BDD evaluation \\
\textit{lift} & Maps expression to ADD semantics &			  \textit{lift} & \textit{lift} \\
Variability encoding ($\gamma$) & Maps compositional into annotative representation &			  Variability encoding  & Lifted composition \\
ADD application ($\llbracket \_ \rrbracket_c$) & Property value mapped by a configuration &			  ADD application   & BDD application \\
			  
			\bottomrule
		\end{tabular}
	}
	\label{table:analysis-abstraction-framework}
\end{table}
%\end{landscape}

% \subsection{Mechanized Formalization in PVS}
\subsection{Formalization}
\label{sec:abstraction-description}

%recalling problem: not being able to describe precisely and uniformly product-line analyses and their properties.
To address our research problem, we developed a machine-verified theory comprising formal specification and verification of key concepts and properties of product-line analyses.
This theory is specified and checked using the PVS proof assistant~\cite{PVS:language}, to aid specification and avoid unsound proof steps, since manual demonstrations are prone to human mistake.
In particular, this formalization defines abstract functions and types modeling essential abstractions in this problem domain such as analysis steps, models, and intermediate analysis results.
Hence, the details of concrete representations, such as reliability analysis using PMC or behavioral analysis over FTS, are abstracted.
The mechanization also provides machine-verified soundness proofs of key results, such as commutativity of different analysis strategies.
This way, we increase confidence that the framework can be reused to safely establish different product-line analysis strategies for specific models and properties.

\subsubsection{PVS}
PVS provides mechanized support for formal specification and verification, with a specification language and a theorem prover. Specifications consist of collections of theories. Each theory consists of signatures for the types introduced in the theory, and the axioms, definitions, and theorems associated with the signature. Specifications are strongly typed; every expression has an associated type. The specification language is based on classical, typed higher-order logic. 

PVS also provides mechanisms such as theory parameterization and interpretation, enabling us to consider variability when specifying our theories. We may use parameters when defining a theory, which provides support for \emph{universal polymorphism}. 
PVS offers separate mechanisms for importing a theory with axioms, and for interpreting a theory by supplying a valid interpretation, that is, one that satisfies its axioms~\cite{theory-interpretations-pvs}. 
The theory interpretation mechanism enables us to show that a theory is correctly interpreted by another theory under a user-specified interpretation for the uninterpreted types and constants. 
We can use interpretations to show that an implementation is a correct refinement of a specification, that an axiomatically defined specification is consistent, or that an axiomatically defined specification captures its intended models. Axioms defined in the theory being interpreted generate proof obligations, to ensure consistency. 

In what follows, we present our formalization using a simplified notation, abstracting from PVS syntax to facilitate the communication of ideas and knowledge.
We first present general framework definitions, and then proceed with formalization of annotative product lines, which forms the basis for specifying compositional product lines.
The full PVS mechanization, comprising all definitions and proofs, is available at a public repository.\footnote{\url{https://github.com/thiagomael/spl-analyses-mechanization/}}

\subsubsection{General Framework Definitions}

We define \pvscode{Product} as a variability-free model defining the type of the product line instances. 
We do so by using uninterpreted types in PVS, which are a way of introducing types with almost no constraints, other than the fact that such types are disjoint from all other types. 
Products have a computable \pvscode{Property} of interest such as reliability. In existing analyses, this property usually has a numerical or Boolean type~\cite{Thum2014}---%e.g., reliability, safety, and performance. 
reliability, safety, performance, etc.
To increase generality, as we do not need to fix a particular type for the property, we also define it using uninterpreted types. 
We introduce special elements \pvscode{emptyproduct \in Product} and \pvscode{emptyproperty \in Property}, to be used as base cases for recursive functions in the compositional model evaluation.
We use type \pvscode{Conf} to represent a configuration. For generality, we also use uninterpreted types to abstract the specific syntax of product configuration. For instance, it could be a set of selected features from a product line. 
Finally, function $\alpha$ is a variability-free analysis that performs the computation of a \pvscode{Property} for a given \pvscode{Product}, such as reachability probability analysis for reliability (cf. Section~\ref{sec:reliability}). 
This function is also uninterpreted, but it must obey the constraint that analyzing an \pvscode{emptyproduct} yields \pvscode{emptyproperty}.
%<<as far as I remember, this became necessary in a proof step. Not sure if we can summarize at this point here a brief motivation. Just a thought, it might as well remains as it is.  A likely intuition is that Property and Product are isomorphic, which only becomes clearer late.>>
% \begin{lstlisting}
%  Product: TYPE+ 
%  emptyproduct: Product
%  nonemptyproduct?(p:Product) : bool = p /= emptyproduct
%  Property: TYPE+
%  emptyproperty : Property
%  nonemptyproperty?(p:Property) : bool = p /= emptyproperty
%  Conf: TYPE+ 
 
%  (*@$\alpha$@*): {f:[Product -> Property] | f(emptyproduct)=emptyproperty}
% \end{lstlisting}

\begin{definition}[Computing Properties from Products]
\label{defn:alpha}
We compute a $Property$ from a $Product$, using a function $\alpha : Product \rightarrow Property$, such that $\alpha(emptyproduct)=emptyproperty$.
% \begin{equation*}
% \begin{split}
% & \alpha : \{f:[Product \rightarrow Property]\;|\; f(emptyproduct)=emptyproperty\}
% \end{split}
% \end{equation*}
\end{definition} 


%\pvscode{Product}, \pvscode{Property}, and $\alpha$ are in bold in Table~\ref{table:analysis-abstraction-framework} and in Figure~\ref{fig:strategies-generic} 
%because they are the hotspots framework. 
%The configurability space and the instantiation of the framework are presented and discussed in Sections~\ref{sec:pl2ana} and~\ref{framework:discussion}.  


%TODO @lmt maybe use Annotative Product Line to refer that the RHS is a feature model + a variant-rich model





%Moreover, the conceptual framework is then used to derive a new, previously unknown, analysis strategy: "feature-family-product analysis".



\subsubsection{Annotative Product Lines}

On the right-hand side of the diagram in \Cref{fig:strategies-generic}, \textit{Annotative Product Line} comprises a feature model, configuration knowledge, and a variant-rich model called \textit{annotative model}. The latter is an extension of \pvscode{Product} including optional presence conditions to annotate model elements.
Therefore, \pvscode{AnnotativeModel} is either a base variability-free \pvscode{Product} model, or a variation point with choices depending on a presence condition. 
Specifically, we use abstract data types to represent this model, in which we provide a set of type constructors, such as \pvscode{ModelBase} and \pvscode{ModelChoice}, along with associated accessors, which allow us to extract arguments from the constructors, such as the product, or the presence condition. \Cref{defn:annotative-model} presents this data type in a simplified way.
% Finally, recognizers, such as \pvscode{ModelBase?}, are predicates over the datatype that are true when their argument is constructed using the correspondent constructor.
%TODO do we need to detail what a PresenceCondition is?
% \begin{lstlisting}
% AnnotativeModel: DATATYPE
%   BEGIN
%       ModelBase(m:Product): ModelBase?
%       ModelChoice(pc:PresenceCondition,
%                   vm1:AnnotativeModel,
%                   vm2:AnnotativeModel): ModelChoice?
%   END AnnotativeModel
% \end{lstlisting}

\begin{definition}[Annotative Model]
\label{defn:annotative-model}
An annotative model is either:
\begin{enumerate}
    \item $ModelBase(m:Product)$, denoting a variability-free product model; or
    \item $ModelChoice(pc:PresenceCondition, vm_1:AnnotativeModel, vm_2:AnnotativeModel)$, denoting variations according to a presence condition. 
\end{enumerate}
% \begin{equation*}
% \begin{split}
% & AnnotativeModel = \\ 
% & ModelBase(m:Product)\\
% & |\;\,ModelChoice(pc:PresenceCondition,vm1:AnnotativeModel,vm2:AnnotativeModel)
% \end{split}
% \end{equation*}
\end{definition} 

The intuition behind this specification is that \pvscode{AnnotativeModel} is a data structure representing the \emph{semantics} of an annotative product line.
That is, an \pvscode{AnnotativeModel} can be seen as a decision tree in which the internal nodes are presence conditions (to be checked against a given configuration) and the leaf nodes are all possible products.
For instance, \Cref{fig:annotative-model-example} illustrates how an annotative model of a product line (the example PMC from \Cref{fig:family-product-example}) could be interpreted according to this concept.

\begin{figure}[!htbp]
	\centering
	\begin{subfigure}[t]{\textwidth}
	\centering
	    \includegraphics{figures/annotative-model-example-base.tikz}
	    \caption{Concrete annotative PMC, denoting the behavioral model of a product line}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
	\centering
	    \includegraphics{figures/annotative-model-example.tikz}
	    \caption{Variational semantics (\pvscode{AnnotativeModel}) of the same PMC, represented as a decision tree}
	\end{subfigure}
	\caption{Intuition of how an annotative model satisfies our specification (assuming that $x$ is bound to $1$ if the corresponding feature is selected, and $0$ otherwise)}
	\label{fig:annotative-model-example}
\end{figure}

Note that such a definition is not meant to be directly implemented; otherwise, modeling a product line would require that all products be modeled individually, which defeats the whole purpose of product line engineering.
However, specification-wise, having an abstraction of all possible products helps on stating and proving theorems that quantify (universally or existentially) over the solution space.

Using this semantic notion of an \pvscode{AnnotativeModel} as a decision tree, product derivation becomes a matter of evaluating presence conditions until a leaf node (i.e., product) is reached.
This product derivation process is denoted by function $\pi$, which receives an annotative model and a configuration, and yields a product.
%performs product derivation on annotative product lines by presence condition evaluation, then  projection and pruning of its underlying annotative model under a given configuration, obtaining a variability-free model representing a derived product of the product line.
% The \pvscode{MEASURE} clause provides an indicator for recursion termination, which is required by PVS.

% \begin{lstlisting}[mathescape=true]
% $\pi$(vm:AnnotativeModel,c:Conf): RECURSIVE Product =
%   CASES vm OF
%       ModelBase(m): m,
%       ModelChoice(pc, vm1, vm2):
%                     IF c(pc) THEN $\pi$(vm1,c)
%                              ELSE $\pi$(vm2,c)
%                     ENDIF
%   ENDCASES
%   MEASURE vm by <<
% \end{lstlisting}
\begin{definition}[Product Derivation from Annotative Models]
\label{defn:pi}
Given an annotative model and a configuration $c$, product derivation is performed by a function $\mathit{\pi:AnnotativeModel \to Conf \to Product}$, such that:
\begin{enumerate}
    \item $\mathit{\pi(ModelBase(m),c) = m}$; and
    \item $\mathit{\pi(ModelChoice(pc,vm_1,vm_2),c)} = \begin{cases}
            \mathit{\pi(vm_1,c)} &\mbox{if} \; c \models pc  \quad \text{\textit{(presence condition is satisfied)}} \\
            \mathit{\pi(vm_2,c)} &\mbox{otherwise}
        \end{cases}$
\end{enumerate}

% \begin{equation*}
% \begin{split}
% & \quad \pi(ModelBase(m),c) = m\\
% & \quad \pi(ModelChoice(pc,vm1,vm2),c) = \\
% & \qquad \pi(vm1,c)\text{, when $pc$ is true according to $c$}\\
% & \qquad \pi(vm2,c)\text{, otherwise}
% \end{split}
% \end{equation*}
\end{definition} 

Similar to annotative models, annotative expressions are specified using a representation of choice semantics.
Hence, we use the \pvscode{BaseExpression} constructor for leaf nodes
(denoting property values of products, such as the bottom-left corner of \Cref{fig:family-product-example})
and \pvscode{ChoiceExpression} to introduce decision nodes.

% \begin{lstlisting}
% AnnotativeExpression: DATATYPE
%   BEGIN
%       BaseExpression(p:Property): BaseExpression?
%       ChoiceExpression(pc:PresenceCondition,
%                       ae1:AnnotativeExpression,
%                       ae2:AnnotativeExpression): ChoiceExpression?
%   END AnnotativeExpression
% \end{lstlisting}

\begin{definition}[Annotative Expression]
\label{defn:annotative-expression}
An annotative expression is either:
\begin{enumerate}
    \item $\mathit{BaseExpression(p\!:Property)}$, denoting a property computed from a product; or
    \item $\mathit{ChoiceExpression(pc\!:PresenceCondition,e_1\!:AnnotativeExpression,e_2\!:AnnotativeExpression)}$, denoting choices guarded by a presence condition. 
\end{enumerate}
% \begin{equation*}
% \begin{split}
% & AnnotativeExpression = \\ 
% & BaseExpression(p:Property)\;\,| \\
% & ChoiceExpression(pc:PresenceCondition,e1:AnnotativeExpression,e2:AnnotativeExpression)
% \end{split}
% \end{equation*}
\end{definition} 

\begin{figure}[!htbp]
	\centering
    \includegraphics{figures/annotative-expression-example.tikz}
    \caption{Variational semantics (\pvscode{AnnotativeExpression}) of the expression $0.9801\cdot x + 0.99\cdot(1-x)$ in \Cref{fig:family-product-example}, represented as a decision tree (assuming that $x$ is bound to $1$ if the corresponding feature is selected, and $0$ otherwise)}
	\label{fig:annotative-expression-example}
\end{figure}

\Cref{fig:annotative-expression-example} illustrates the intuition of an \pvscode{AnnotativeExpression} encoded in this way.
Such annotative expressions encode the semantics of property values in terms of product-line configurations, so that annotative expression evaluation is defined recursively as we did for product derivation.%, as follows:
% TODO vra: consider inlining the example into the background section or extending the one in Figure~\ref{fig:family-product-example}

% \begin{lstlisting}[mathescape=true]
% $\sigma$(vp:AnnotativeExpression,c:Conf): RECURSIVE Property =
%   CASES vp OF
%       BaseExpression(p): p,
%       ChoiceExpression(pc, vp1, vp2):
%             IF c(pc) THEN $\sigma$(vp1,c)
%                      ELSE $\sigma$(vp2,c)
%             ENDIF
%   ENDCASES
%   MEASURE vp by << 
% \end{lstlisting} 

\begin{definition}[Annotative Expression Evaluation]
\label{defn:sigma}
Given an annotative expression and a configuration $c$, evaluation is performed by a function $\mathit{\sigma:AnnotativeExpression \to Conf \to Property}$, such that:
\begin{enumerate}
    \item $\mathit{\sigma(BaseExpression(p),c) = p}$; and
    \item $\mathit{\sigma(ChoiceExpression(pc,e_1,e_2),c)} = \begin{cases}
            \sigma(e_1,c) &\mbox{if} \; c \models pc  \quad \text{\textit{(presence condition is satisfied)}} \\
            \sigma(e_2,c) &\mbox{otherwise}
        \end{cases}$
\end{enumerate}
% \begin{equation*}
% \begin{split}
% & \sigma: [AnnotativeExpression,Conf \rightarrow Property]\\ 
% & \quad \sigma(BaseExpression(p),c) = p\\
% & \quad \sigma(ChoiceExpression(pc,e1,e2),c) = \\
% & \qquad \sigma(e1,c)\text{, when $pc$ is true according to $c$}\\
% & \qquad \sigma(e2,c)\text{, otherwise}
% \end{split}
% \end{equation*}
\end{definition}

Since the data types for both annotative models and annotative expressions denote the semantics of the corresponding artifacts, we specify function $\hat{\alpha}$ (variability-aware analysis on annotative models) by recursively mapping their structures. %It incorporates knowledge about variability and explores commonalities (sharing) in such models. 
%In particular, we employ a generative approach whereby we derive $\hat{\alpha}$ from $\alpha$ by analyzing an annotative model via structural recursion, lifting variability from the structure of the model to the structure of annotative expressions representing values of the property.
In the case that the model has no variability, a  variability-free expression (i.e., the result of applying $\alpha$ to the model) is returned. Otherwise, an annotative choice expression is yielded.

% \begin{lstlisting}[mathescape=true]
%  $\hat\alpha$(vm:AnnotativeModel): RECURSIVE AnnotativeExpression =
%   CASES vm OF
%       ModelBase(m): BaseExpression($\alpha$(m)),
%       ModelChoice(pc,vm1,vm2): ChoiceExpression(pc, $\hat\alpha$(vm1), $\hat\alpha$(vm2))
%   ENDCASES
%  MEASURE vm by <<
% \end{lstlisting}

\begin{definition}[Variability-aware Analysis on Annotative Models]
\label{defn:hat-alpha}
Given an annotative model, we use the function $\mathit{\hat{\alpha}:AnnotativeModel \to AnnotativeExpression}$ to perform variability-aware analysis, such that:
\begin{enumerate}
    \item $\mathit{\hat\alpha(ModelBase(m)) = BaseExpression(\alpha(m))}$; and
    \item $\mathit{\hat\alpha(ModelChoice(pc,vm_1,vm_2)) = ChoiceExpression(pc,\hat\alpha(vm_1),\hat\alpha(vm_2))}$.
\end{enumerate} 
% \begin{equation*}
% \begin{split}
% & \hat\alpha: [AnnotativeModel \rightarrow AnnotativeExpression]\\ 
% & \quad \hat\alpha(ModelBase(m)) = BaseExpression(\alpha(m))\\
% & \quad \hat\alpha(ModelChoice(pc,vm1,vm2)) = ChoiceExpression(pc,\hat\alpha(vm1),\hat\alpha(vm2))
% \end{split}
% \end{equation*}
\end{definition}

Furthermore, the structure of the top-right quadrant in \Cref{fig:strategies-generic}  means that, for a given configuration \pvscode{conf}, the evaluation of the annotative expression obtained by $\hat{\alpha}$ from an annotative model \pvscode{vModel} yields the same result as if the variability had first been bound for such configuration, then the resulting product analyzed via the non-variability-aware analysis $\alpha$. 
Formally specifying and proving this key property of family-based analyses is often overlooked~\cite{Thum2014}. 
% We encoded it as a theorem in PVS as follows:
% ??? Formally specifying and proving this key property of family-based analyses is often overlooked~\cite{Thum2014}, error-prone, and time consuming, specially considering that it is accomplished repeatedly for concrete models and specific properties. We encoded this key property for abstract model and property as a theorem in PVS as follows:
%This is a key property of family-based analyses and is encoded as a theorem in PVS as follows:

%TODO @ALL discuss whether we should use math symbols on alpha/sigma
%TODO @lmt check with Thiago which environment are we going to use for theorems and proofs
% \begin{lstlisting}[mathescape=true]
% commutative_family_product_product: THEOREM
%     $\forall_{\mathtt{vModel}, \; \mathtt{conf}}$ $\cdot$ $\sigma$($\hat\alpha$(vModel),conf) = $\alpha$($\pi$(vModel,conf))
% \end{lstlisting} 

\begin{theorem}[Soundness of family-product-based analysis]
\label{thm:soundness-family-product}
\begin{equation*}
\begin{split}
& \forall_{\mathit{vModel}, \; \mathit{conf}} \cdot \sigma(\hat\alpha(\mathit{vModel}),\mathit{conf}) = \alpha(\pi(\mathit{vModel},\mathit{conf}))
\end{split}
\end{equation*}
\end{theorem}
% TODO @lmt. Could you please summarize below?
\begin{proof}[Proof sketch]
 For an arbitrary configuration \pvscode{conf}, we need to prove that 
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(vModel),conf) = $\alpha$($\pi$(vModel,conf))} 
$\mathit{\sigma(\hat{\alpha}(vModel),conf)} = \mathit{\alpha(\pi(vModel,conf))}$ 
 holds for any annotative model \pvscode{vModel}. 
 We prove this by structural induction over \pvscode{vModel}. 
 This generates two subgoals, which correspond to the possibilities for the \pvscode{AnnotativeModel} data type. 
 In the first subgoal, we need to prove that 
 $\mathit{\sigma(\hat{\alpha}(ModelBase(m)),conf)} = \mathit{\alpha(\pi(ModelBase(m)),conf)}$.
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(ModelBase(m)),conf) = $\alpha$($\pi$(ModelBase(m),conf))}.
 By expanding $\hat\alpha$ on the left-hand side (LHS), and product derivation $\pi$ in the right-hand side (RHS), we then have to prove that 
 $\mathit{\sigma(BaseExpression(\alpha(m)),conf)} = \alpha(m)$.
%  \lstinline[mathescape=true]{$\sigma$(BaseExpression($\alpha$(m)),conf) = $\alpha$(m)}.
 We then expand $\sigma$ on the LHS, resulting on 
 $\alpha$(m) = $\alpha$(m),
%  \lstinline[mathescape=true]{$\alpha$(m) = $\alpha$(m)},
 which is trivially true. 
 
 In the second subgoal, for arbitrary presence condition \pvscode{pc} and annotative models \pvscode{am_1} and \pvscode{am_2}, we must prove that 
 $\mathit{\sigma(\hat\alpha(ModelChoice(pc, am_1, am_2)), conf)} = \mathit{\alpha(\pi(ModelChoice(pc, am_1, am_2), conf))}$.
 Also, $\mathit{\sigma(\hat\alpha(am_1), conf)} = \mathit{\alpha(\pi(am_1, conf))}$
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(am_1), conf) = $\alpha$($\pi$(am_1, conf))} 
 and 
 $\mathit{\sigma(\hat\alpha(am_2), conf)} = \mathit{\alpha(\pi(am_2, conf))}$ are induction hypotheses.
% \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(am_2), conf) = $\alpha$($\pi$(am_2, conf))}.
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(ModelChoice(pc, am1, am_2)), conf) = $\alpha$($\pi$(ModelChoice(pc, am_1, am_2), conf))}. 
 Expanding $\sigma$ and $\hat\alpha$ on the LHS, and $\pi$ on the RHS of the proof goal, we then have two possible situations, corresponding exactly to the two induction hypotheses.
 If \pvscode{conf} satisfies the presence condition \pvscode{pc}, we must prove that
 $\mathit{\sigma(\hat\alpha(am_1), conf)} = \mathit{\alpha(\pi(am_1, conf))}$, 
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(am1), conf) = $\alpha$($\pi$(am1, conf))}, 
 which is already given by one of the induction hypotheses.
 Otherwise, we have to prove 
 $\mathit{\sigma(\hat\alpha(am_2), conf)} = \mathit{\alpha(\pi(am_2, conf))}$, 
%  \lstinline[mathescape=true]{$\sigma$($\hat\alpha$(am_2), conf) = $\alpha$($\pi$(am_2, conf))}, 
 concluding the proof of this subgoal since this is also given by an induction hypothesis.
%Finally, in the third subgoal, for arbitrary annotative models \pvscode{am1} and \pvscode{am2}, by the induction hypothesis we have that \pvscode{sigma(hatAlpha(am1), conf) = alpha(pi(am1, conf))} and \pvscode{sigma(hatAlpha(am2), conf) = alpha(pi(am2, conf))}.
 %We then have to prove that \pvscode{sigma(hatAlpha(ModelComposite(am1, am2)), conf) = alpha(pi(ModelComposite(am1, am2), conf))}.  By expanding \pvscode{sigma} and \pvscode{hatAlpha} on the LHS, and \pvscode{alpha} and \pvscode{pi} on the RHS, we then have to prove that \pvscode{analyzeModelCompositeShell(sigma(hatAlpha(am1), conf), sigma(hatAlpha(am2), conf)) = analyzeModelCompositeShell(alpha(pi(am1, conf)), alpha(pi(am2, conf)))}. This is trivially true given the two induction hypothesis.
\end{proof}
 
%@vra: we deliberately omit explicit reference to  Compositional Parametric Model, since it is not 
% in the diagram, yet it is present in the SCP theory.

%TODO @lmt maybe use Compositional Product Line to refer that the LHS is a feature model + a variant-rich model?

%  <<just a thought to discuss in a meeting ( I also had a chat with Sven): we could move the initial bullets of Section 4.4 here to help with intuition or we could add something like this here (perhaps a bit abstract indeed) or add later in Section 4.4: 
% "Essentially, the compositional side of the framework is a foldable functor of homomorphic monoids: at a fine-grained level, the monoids model annotative models and expressions equipping them with associative composition operations and an identity element. The monoids are homomorphic to allow compositional analysis along the functor structure, which is also foldable to allow binding or encoding of variability.">>

At this point, it is important to note that $\hat{\alpha}$ itself maps the semantics of an annotative model into the semantics of a corresponding annotative expression \emph{by construction}.
In other words, the framework assumes that each \pvscode{BaseProduct} $p$ (leaf node in an \pvscode{AnnotativeModel}) is mapped to a corresponding \pvscode{BaseExpression} $\alpha(p)$ (leaf node in an \pvscode{AnnotativeExpression}).
Therefore, to instantiate the framework, one is \emph{required} to prove that such assumption holds for the concrete models and the concrete analysis at hand.

\subsubsection{Compositional Product Lines}
\label{sec:compositionalSPL}

On the left-hand side of the diagram in \Cref{fig:strategies-generic}, \textit{Compositional Product Line} also comprises a feature model, configuration knowledge, and a variant-rich model called \textit{compositional model}, which is a structure of related annotative models that compose as specified next.

%For instance, a compositional model is depicted on top-right side of Figure~\ref{fig:functor} where each square could represent an annotative model like the one on the top-left side of Figure~\ref{fig:family-product-example}. 
% \begin{lstlisting}[mathescape=true]
% CompositionalModel: TYPE = [# 
%   idt : $\mathcal{F}$[$\mathbb{N}$],
%   E : [$\mathbb{N} \rightarrow$ AnnotativeModel],
%   ord : <,
%   top : (idt)
% #]
% \end{lstlisting}

\begin{definition}[Compositional Model]
\label{defn:compositional-models}
A compositional model is a tuple $\mathit{(idt, E, \prec, top)}$, where:
\begin{itemize}
    \item \pvscode{idt} is a finite set of natural numbers, meant to be \emph{identifiers};
    \item $E: \mathbb{N} \to \mathit{AnnotativeModel}$ is a function that maps identifiers in \pvscode{idt} to \pvscode{AnnotativeModel}s;
    \item $\prec$ is a well-founded dependency relation between identifiers from \pvscode{idt};
    \item $\mathit{top} \in \mathit{idt}$ denotes the identifier of the \emph{root} model---i.e., a model on which no other depends.
\end{itemize}
% \begin{equation*}
% \begin{split}
% & CompositionalModel =\;\prec idt : \mathcal{F}[\mathbb{N}], E : [\mathbb{N} \rightarrow AnnotativeModel], ord : <, top : (idt) \succ
% \end{split}
% \end{equation*}
\end{definition}

% Where \pvscode{idt} is a finite set of natural numbers, \pvscode{E} is a mapping from such set to  elements of type \pvscode{AnnotativeModel}, \pvscode{ord} is a well-founded relation among the set \pvscode{idt}, and \pvscode{top} is an element from \pvscode{idt} to denote the root element of the \pvscode{CompositionalModel}. These elements are accessed through the notation \lstinline{cm`top}, for instance. 

A compositional model is a named finite set of annotative models with an associated dependency relation between them, which denotes the possible compositions.
% To denote this structure, we use a finite set of Natural identifiers (\pvscode{idt}) and a named set of elements, represented by a function that maps a Natural number to an \pvscode{AnnotativeModel} that is identified by it (\pvscode{E}).
% Moreover, we also represent dependencies by means of a well-founded relation between identifiers (\pvscode{ord}), denoting by \pvscode{top} the identifier of the \emph{root} model---i.e., a model on which no other depends.
For a \pvscode{CompositionalModel} \pvscode{cm}, \pvscode{cm'top} identifies the base for composition.
%
The members of a \pvscode{CompositionalModel} have type \pvscode{AnnotativeModel} to denote that, in a concrete setting, such artifacts are expected to carry some sort of annotation that indicates where to perform composition.
These annotations can be placeholders as in \Cref{fig:feature-product-example}, for instance.
In some contexts, such as feature-oriented programming~\cite{fop}, it may seem that models should be specified as regular products; however, feature modules are essentially slices with a different semantics for \texttt{super} (which refers to the same method in another module) and \texttt{class} (which is actually a partial class or mixin).
In general, previous work~\cite{gamaLJ} explores integration strategies for annotative and compositional mechanisms. 
% To motivate the fact that \pvscode{E} maps to \pvscode{AnnotativeModel} instead of \pvscode{Product}, we note that, for instance, in feature-oriented programming~\cite{fop}, although feature modules look like normal modules, they are essentially slices with a different semantics for \pvscode{super} (refers to the same method in another module) and \pvscode{class} (actually a partial class or mixin).
% In general, previous work~\cite{gamaLJ} explores integration strategies for annotative and compositional mechanisms. 


We exploit the dependency relation among the constituent elements of a \pvscode{CompositionalModel} to define the \pvscode{dependents} function. This function takes as input a \pvscode{CompositionalModel} and an identifier $i$ for a particular annotative model therein, and yields a list of dependents of such model, consisting of a pair $(pc,id)$, formed by the \pvscode{id} of the dependent model and its presence condition.
The latter can be obtained from the structure of an \pvscode{AnnotativeModel} through the \pvscode{getPC} function. 
% We use set notation to express the \pvscode{dependents} function to ease comprehension. 
%Moreover, we use $p'pc$ to denote that we are accessing the presence condition $pc$ from the pair $p$.

% \begin{lstlisting}[mathescape=true]
% Pair: TYPE =  [# pc : PresenceCondition, idt : $\mathbb{N}$ #]
% dependents(cm:CompositionalModel, id:{ n:$\mathbb{N}$ | n $\in$ cm`idt}) : list[Pair] =
%  {p:Pair | $\exists$(pc:PresenceCondition,n:$\mathbb{N}$): n$<$id $\wedge$ p`idt = n $\wedge$ p`pc = getPC(cm,n)}
% \end{lstlisting}

%<\!\!\!\!< >\!\!\!\!>
\begin{definition}[Extracting dependent models]
\label{defn:pair-dependents}
Given a compositional model $cm$ and a natural identifier $i \in \mathit{cm'idt}$, the function $\mathit{dependents}$ yields a list of pairs $\mathit{(pc,id)}$ such that $\mathit{pc = getPC(cm,id)}$ and $id \prec i$.
% \[\exists(pc:PresenceCondition,n:\mathbb{N}): n<i \wedge p'idt = n w\wedge p'pc = getPC(cm,n).\]
% \begin{equation*}
% \begin{split}
% & Pair =\;\prec pc : PresenceCondition, idt : \mathbb{N} \succ \\
% & dependents(cm:CompositionalModel, id:\{ n:\mathbb{N} | n \in cm`idt\}) : list[Pair] =\\
% & \quad \{p:Pair\,|\,\exists(pc:PresenceCondition,n:\mathbb{N}): n<id \wedge p'idt = n \wedge p'pc = getPC(cm,n)\}\\
% % & partialModelComposition : [AnnotativeModel, AnnotativeModel \rightarrow AnnotativeModel]
% \end{split}
% \end{equation*}
\end{definition}

To either bind or encode variability in compositional models, basic behavior is needed for the composition of elements in each node of the relation, incorporating dependent elements from the recursive composition along the structure. Therefore, we assume the existence of the function  \pvscode{partialModelComposition:AnnotativeModel \to AnnotativeModel \to AnnotativeModel}, which binds the composition mechanism for \pvscode{AnnotativeModel}. 

% To either bind or encode variability in compositional models, basic behavior is needed for the composition of elements in each node of the relation, incorporating dependent elements from the recursive composition along the structure. Therefore, we assume the existence of  \pvscode{partialModelComposition}, which binds the composition mechanism for \pvscode{AnnotativeModel}. 
% \begin{lstlisting}[mathescape=true]
% partialModelComposition: [AnnotativeModel, AnnotativeModel -> AnnotativeModel]
% \end{lstlisting}
%Derivation by \textit{composition} ($\pi'$) then operates on compositional product lines generating a product by composing the annotative models within the compositional model of the product line for a given configuration of its feature model. 
Derivation by \textit{composition} ($\pi'$) then operates on compositional product lines generating a product for a given configuration of its feature model. In particular, $\pi'$ relies on auxiliary function $\pi_r'$ to recursively 
perform a bottom-up composition of bound annotative models within the compositional model of the product line by leveraging the \pvscode{partialModelComposition} function.
Since the dependency relation is well-founded, this recursion is guaranteed to terminate.

% \begin{lstlisting}[mathescape=true]
% $\pi'$(cm:CompositionalModel,c:Conf): Product = $\pi_r'$(cm,cm`top,c)
% $\pi_r'$(cm:CompositionalModel,idt:(cm`idt),c:Conf): RECURSIVE Product =
%     $\pi$(
% 	  foldl(
% 	      partialModelComposition,
% 	      cm`E(idt),
% 	      map(
% 	        LAMBDA(p:Pair | p $\in$ dependents(cm, idt)) : 
% 	        IF ( c(p`pc) ) THEN 
% 	          ModelBase($\pi_r'$(cm, p`idt, c)) 
% 	        ELSE 
% 	          ModelBase(emptyproduct) ENDIF,
% 	        dependents(cm, idt)
% 	      )
% 	  ),
%       c
%     )
%     MEASURE idt BY cm`ord
% \end{lstlisting}
\begin{definition}[Derivation by composition]
\label{defn:pi-prime}
Given a compositional model $cm$ and a configuration $c$, product derivation is performed by a function $\mathit{\pi': CompositionalModel \to Conf \to Product}$, such that $\mathit{\pi'(cm,c)=\pi_r'(cm,cm`top,c)}$. 
The auxiliary function $\pi_r'$ expects a compositional model $cm$, a Natural identifier $i \in \mathit{cm'idt}$, and a configuration $c$, such that:
\[\mathit{\pi_r'(cm,i,c)} = \mathit{\pi(foldl(partialModelComposition, cm'E(i), map(f,dependents(cm,i))),c)}\] where $f$ receives a pair $(pc,idt) \in \mathit{dependents(cm, i)}$ as an argument, and is defined as:
\[\mathit{f(pc,idt)}=\begin{cases}
            \mathit{ModelBase(\pi_r'(cm, idt, c))} &\mbox{if} \; c \models pc \\
            \mathit{ModelBase(emptyproduct)} &\mbox{otherwise}
        \end{cases}\]
% \begin{equation*}
% \begin{split}
% & \pi_r'(cm,i,c) = \\
% & \quad \pi ( foldl(\\
% & \qquad partialModelComposition,\\
% & \qquad cm`E(idt),\\
% & \qquad map(\lambda (pc,idt) \in dependents(cm, i) : )
% \end{split}
% \end{equation*}
\end{definition}

In the same vein, compositional expressions are defined as the \pvscode{CompositionalExpression} type akin to \pvscode{CompositionalModel}.
The only difference is that field \pvscode{E} now maps to \pvscode{AnnotativeExpression} instead of \pvscode{AnnotativeModel}.
We also obtain the list of dependent expressions through \pvscode{dependents}, and \pvscode{partialExpComposition} binds the composition mechanism for \pvscode{AnnotativeExpression}.
Evaluation of a \pvscode{CompositionalExpression} is given by 
$\sigma'$, 
% \lstinline[mathescape=true]{$\sigma'$}, 
which operates similarly as 
$\pi'$.
% \lstinline[mathescape=true]{$\pi'$}.
For brevity, we omit these definitions, which can be found in the mechanization repository.


% \begin{lstlisting}[mathescape=true]
% CompositionalExpression : TYPE = [# 
%   idt : $\mathcal{F}$[$\mathbb{N}$],
%   E : [$\mathbb{N} \rightarrow$ AnnotativeExpression],
%   ord : <,
%   top : (idt)
% #]
% \end{lstlisting}
% Where \pvscode{idt} is a finite set of natural numbers, \pvscode{E} is a mapping from such set to  elements of type \pvscode{AnnotativeExpression}, \pvscode{ord} is a well-founded relation among the set \pvscode{idt}, and \pvscode{top} is an element from \pvscode{idt} to denote the root element of the \pvscode{CompositionalExpression}. \textbf{Repeated - do we need to introduce the type explicitly or just mention that it is similar to CompositionalModel - We also have dependents for compositional expression}


%Pair : TYPE =  [# pc : PresenceCondition, idt : nat #]


%\begin{lstlisting}[mathescape=true]
%$\sigma'$(ce:CompositionalExpression,c:Conf): Property = $\sigma_r'$(ce,ce`top,c)

%$\sigma_r'$(ce:CompositionalExpression,idt:(ce`idt),c:Conf): RECURSIVE Property =
%  $\sigma$(
%    foldl(
%      partialExpComposition,
%      ce`E(idt),
%      map(
%        LAMBDA(p:Pair | p $\in$ dependents(ce, idt)) : 
%          IF ( c(p`pc) ) THEN 
%            BaseExpression($\sigma_r'$(ce, p`idt, c)) 
%          ELSE 
%            BaseExpression(emptyproperty) 
%          ENDIF,
%          dependents(ce, idt)
%      )
%    ),
%    c
%  )
%  MEASURE idt BY ce`ord
%\end{lstlisting}

% %connecting the interface of one  compositional parametric model to the slot of another model. 
% % vra: commented below to avoid Sven's issues due to lack of detail at this point.
% %The derivation terminates because the structure (relation) on the elements is assumed to be well-founded.

\hyphenation{partial-Exp-Composition}

The correspondence between \pvscode{CompositionalExpression} and \pvscode{CompositionalModel} is a foundation for applying $\hat{\alpha}$ to map component models into corresponding expressions, preserving the structure implied by the dependency relation.
Moreover, placeholders in a model should have corresponding markers in the mapped expression, so as to preserve composition semantics.
However, the mechanism by which \pvscode{partialModelComposition} and \pvscode{partialExpComposition} work depends on concrete models; hence, we leave both functions uninterpreted and specify a constraint that needs to be satisfied, given by the assumption below.
Essentially, such assumption means that $\hat{\alpha}$ is compositional.
%In other words, $\hat{\alpha}$ is a homomorphism between \pvscode{AnnotativeModel} and \pvscode{AnnotativeExpression}, where the respective partial compositions preserve $\hat{\alpha}$.

% \begin{lstlisting}[mathescape=true]
% hatAlphaCompositionality: AXIOM
%     $\forall_{\mathtt{m1}, \; \mathtt{m2}}$ $\cdot$ $\hat{\alpha}$(partialModelComposition(m1,m2)) = partialExpComposition($\hat{\alpha}$(m1),$\hat{\alpha}$(m2))
% \end{lstlisting}

\begin{axiom}[Compositionality of $\hat{\alpha}$]
\label{axiom:compositionality-hat-alpha}
\begin{equation*}
\begin{split}
& \forall_{\mathit{m_1}, \; \mathit{m_2}} \cdot \mathit{\hat{\alpha}(partialModelComposition(m_1,m_2))} = \mathit{partialExpComposition(\hat{\alpha}(m_1),\hat{\alpha}(m_2))}
\end{split}
\end{equation*}
\end{axiom}

Similarly to the structure of the top-right quadrant of \Cref{fig:strategies-generic}, the top-left quadrant means that obtaining a compositional expression by mapping $\hat{\alpha}$ over a given compositional model \pvscode{vModel} and then evaluating that expression against a configuration \pvscode{conf} yields the same result as applying $\alpha$ to the product derived from \pvscode{vModel} and \pvscode{conf}. 
%This theorem is specified in PVS as follows:
% \begin{lstlisting}[mathescape=true]
% commutative_feature_product_product: THEOREM
%     $\forall_{\mathtt{vModel}, \; \mathtt{conf}}$ $\cdot$ $\sigma'$(fmap($\hat\alpha$,vModel),conf) = $\alpha$($\pi'$(vModel,conf))
% \end{lstlisting} 
\begin{theorem}[Soundness of feature-product-based analysis]
\begin{equation*}
\begin{split}
& \forall_{\mathit{vModel}, \; \mathit{conf}} \cdot \mathit{\sigma'(\text{fmap}(\hat\alpha,vModel),conf)} = \mathit{\alpha(\pi'(vModel,conf))}
\end{split}
\end{equation*}
\end{theorem}
\begin{proof}[Proof sketch]
 For an arbitrary configuration %\lstinline[mathescape=true]{conf}
 \pvscode{conf}
 and compositional model %\lstinline[mathescape=true]{vModel}, 
 \pvscode{vModel}, we need to prove that 
%  \lstinline[mathescape=true]{$\sigma'$(fmap($\hat\alpha$,vModel),conf) = $\alpha$($\pi'$(vModel,conf))}.
 $\mathit{\sigma'(fmap(\hat\alpha,vModel),conf)} = \mathit{\alpha(\pi'(vModel,conf))}$.
 By expanding %\lstinline[mathescape=true]{$\sigma'$} 
 $\sigma'$ and %\lstinline[mathescape=true]{$\pi'$}, 
 $\pi'$, we then have to prove that 
%  \lstinline[mathescape=true]{$\sigma_r'$(fmap($\hat\alpha$,vModel),fmap($\hat\alpha$,vModel)`top,conf) = $\alpha$($\pi_r'$(vModel,vModel`top,conf))}.
 $\mathit{\sigma_r'(fmap(\hat\alpha,vModel),fmap(\hat\alpha,vModel)'top,conf)} = \mathit{\alpha(\pi_r'(vModel,vModel'top,conf))}$.
By generalizing %\lstinline{vModel`top}
\pvscode{vModel'top} 
and applying well-founded induction, we must prove that, for a given identifier %\lstinline[mathescape=true]{$x \in$ vModel`idt},
$x \in \mathit{vModel'idt}$,
% \lstinline[mathescape=true]{$\sigma_r'$(fmap($\hat\alpha$,vModel),x,conf) = $\alpha$($\pi_r'$(vModel,x,conf))}.
$\mathit{\sigma_r'(fmap(\hat\alpha,vModel),x,conf)} = \mathit{\alpha(\pi_r'(vModel,x,conf))}$.
By expanding %\lstinline[mathescape=true]{$\sigma_r'$}
$\sigma_r'$
and %\lstinline[mathescape=true]{$\pi_r'$},
$\pi_r'$, 
we have to prove that
\begin{equation*}
\begin{split}
& \mathit{\sigma(foldl(partialExpComposition,fmap(\hat\alpha,vModel)'E(x),map(...)),conf)}\\
& = \mathit{\alpha(\pi(foldl(partialModelComposition,vModel'E(x),map(...)),conf))}
\end{split}
\end{equation*}
We omit the contents of the enclosed %\lstinline[mathescape=true]{map}
\pvscode{map} operations inside %\lstinline[mathescape=true]{foldl}
\pvscode{foldl}, for brevity, as their structure can be found in \Cref{defn:pi-prime}. 
% \begin{lstlisting}[mathescape=true]
%     $\sigma$(foldl(partialExpComposition,fmap($\hat\alpha$,vModel)`E(x),map(...)),c)
%     = $\alpha$($\pi$(foldl(partialModelComposition,vModel`E(x),map(...)),c))
% \end{lstlisting}
We reuse the previously proved \Cref{thm:soundness-family-product}, %\lstinline[mathescape=true]{commutative_family_product_product} theorem,
instantiated with %\lstinline[mathescape=true]{foldl(partialModelComposition,vModel`E(x),map(...))}. 
$\mathit{foldl(partialModelComposition,vModel'E(x),map(...))}$.
By replacing that in our goal, we can use \Cref{axiom:compositionality-hat-alpha} % \lstinline[mathescape=true]{hatAlphaCompositionality} axiom 
instantiated with %\lstinline[mathescape=true]{cm`E(x)} 
$\mathit{cm'E(x)}$ 
and the %\lstinline[mathescape=true]{map} 
\pvscode{map} 
operation used in the theorem instantiation (see above). 
We then have to prove the following:  
\begin{equation*}
\begin{split}
& \mathit{\sigma(foldl(partialExpComposition,\hat\alpha(vModel'E(x)),map(...)),conf)}\\
& = \mathit{\sigma(foldl(partialExpComposition,\hat\alpha(vModel'E(x)),map(\hat\alpha,map(...)),conf)}
\end{split}
\end{equation*}
% \begin{lstlisting}[mathescape=true]
%     $\sigma$(foldl(partialExpComposition,$\hat\alpha$(vModel`E(x)),map(...)),c)
%     = $\sigma$(foldl(partialExpComposition,$\hat\alpha$(vModel`E(x)),map($\hat\alpha$,map(...)),c)
% \end{lstlisting}
From the goal, we see that the right-hand side consists of two nested map operations. The outermost being the application of $\hat\alpha$ to the list yielded by mapping over each pair $p$ yielded by $p \in \mathit{dependents(vModel, x)}$. %\lstinline[mathescape=true]{p $\in$ dependents(vModel, x)}. 
On the other hand, the left-hand side has a single map operation, which is applied over each pair $p$ given by $p \in \mathit{dependents(\text{fmap}(\hat\alpha,vModel), x))}$. %\lstinline[mathescape=true]{p $\in$ dependents(fmap($\hat\alpha$,vModel), x))}.
To conclude the proof, we need to establish the equivalence of those map operations, which follows from finite induction on the list of dependents.
\end{proof}

The previous proof relies on \Cref{axiom:compositionality-hat-alpha}, %the \textit{hatAlphaCompositionality} Axiom,
which is an abstraction over the particular types of the model and the non-variability-aware analysis function $\alpha$.
Hence, when these types are instantiated to concrete ones, PVS automatically generates a corresponding theorem (a \emph{proof obligation}) to establish that $\alpha$ is indeed compositional.
This fact highlights one of the key advantages of using PVS: its type system generates theorems such as the said obligation, which could go unnoticed in a handcrafted specification.
While addressing this proof obligation requires some effort, it is inherent to the analysis at hand.
Obligations aside, the proof of the previous theorem on the commutativity of the upper-left corner is completely reusable across different models, properties, and instantiations of non-variability-aware analysis.
